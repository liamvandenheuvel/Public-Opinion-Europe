---
title             : "Public opinion on selective reporting, publication bias, and lack of data sharing in psychological science"
shorttitle        : "PUBLIC OPINION SCIENTIFIC SELECTIVITY"
author: 
  - name          : "Anna E. van 't Veer"
    affiliation   : "1"
    corresponding : yes    
    address       : "Faculty of Social Sciences"
    email         : "a.e.van.t.veer@fsw.leidenuniv.nl"
  - name          : "Maaike L. Verburg"
    affiliation   : "2"
  - name          : "Daniel Lakens"
    affiliation   : "3"
affiliation:
  - id            : "1"
    institution   : "Methodology and Statistics Unit, Psychology institute, Leiden University, ORCID: https://orcid.org/0000-0002-2733-1841"
  - id            : "2"
    institution   : "ORCID: https://orcid.org/0000-0001-9408-3190"
  - id            : "3"
    institution   : "Dept. Industrial Engineering & Innovation Sciences, Eindhoven University of Technology, ORCID: https://orcid.org/0000-0002-0247-239X"
authornote: |
  Contributor roles: Conceptualization: . Data curation: . Formal Analysis: . Funding acquisition: . Investigation: . Methodology: . Project administration: . Resources: . Software: . Supervision: . Validation: . Visualization: . Writing – original draft: . Writing – review & editing: .
  Enter author note here.
abstract: |
  In psychological science, as in other fields, several research practices underlie issues with
  the reliability and integrity of the knowledge produced. In the proposed study we will build
  on a survey study by Picket and Roche (2017) who examined moral judgments about data
  fraud and selective reporting, and found that people in the USA believe both practices to
  be immoral and deserving of punishment. We will assess public opinion in four large
  samples that are quota-equivalent to the OCED statistics in the variables age, sex, and
  where possible education level, in four European countries that differ in their general trust
  in science (Spain, The Netherlands, Italy, and Poland) and will ask participants about three
  research practices in psychological science, namely selective reporting, publication bias,
  and lack of data sharing. Participants will indicate their opinion both before and after the
  incentive structure in science that leads researchers to perform these practices is carefully
  explained. The main outcome measures include moral acceptability of these three
  practices, trust in knowledge produced by Psychological Science (both measured before
  and after the incentive structure explanation), and agreement with tax money going to
  Psychological Science should be lowered if no responsibility is taken to resolve these
  practices. Our hypothesis is that the general public will judge the three practices to be
  morally unacceptable, and we expect to see little change in these judgments after
  informing the public about current reward structures in science. Our goal is to provide input
  for future discussions about policies regarding reporting practices (e.g., pre-registering
  hypothesis tests to prevent p-hacking), publication practices (e.g., study registries in
  psychology to reduce the file-drawer), and data sharing (e.g., sharing data in a data
  repository alongside a publication). If the public believes current research practices to be
  morally unacceptable, the scientific community should work together to improve these
  practices to deserve the trust of the general public. 
  
keywords          : "Public opinion, selective reporting, publication bias, data sharing, [add any other keywords]"
wordcount         : "`r stringr::str_count(rmarkdown::metadata$abstract, '\\S+')`"
bibliography      : ["r-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: 72
---

```{r, papaja, include=FALSE}
options(tinytex.verbose = TRUE)

# The YAML contains a piece of code which was needed to implement APA 7th edition guidelines into Papaja. The implementation of APA 7th edition is still a work in progress. Updates regarding APA 7th edition and Papaja are discussed on Github: https://github.com/crsh/papaja/issues/342

```

```{r, installing packages, include = FALSE, echo = TRUE, message=FALSE}

###################START###################

## Load all packages and install them if needed
## Note: if you need to install papaja, update CRAN-packages only (option 2)

if (!require("checkpoint")) {
  install.packages("checkpoint", repos = "http://cran.us.r-project.org")
  library("checkpoint")
}
if (!require("remotes")) {
  install.packages("remotes", repos = "http://cran.us.r-project.org")
  library("remotes")
}
if (!require("devtools")) {
  install.packages("devtools", repos = "http://cran.us.r-project.org")
  library("devtools")
}
if (!require("dplyr")) {
  install.packages("dplyr", repos = "http://cran.us.r-project.org")
  library("dplyr")
}
if (!require("tidyr")) {
  install.packages("tidyr", repos = "http://cran.us.r-project.org")
  library("tidyr")
}
if (!require("stringr")) {
  install.packages("stringr", repos = "http://cran.us.r-project.org")
  library("stringr")
}
if (!require("scales")) {
  install.packages("scales", repos = "http://cran.us.r-project.org")
  library("scales")
}
if (!require("papaja")) {
  install_github("https://github.com/crsh/papaja")
  library("papaja")
}
if (!require("tidyverse")) {
  install.packages("tidyverse", repos = 'http://cran.us.r-project.org')
  library("tidyverse")
}
if (!require("skimr")) {
  install.packages("skimr", repos = "http://cran.us.r-project.org")
  library("skimr")
}
if (!require("psych")) {
  install.packages("psych", repos = "http://cran.us.r-project.org")
  library("psych")
}
if (!require("tidystats")) {
  install.packages("tidystats", repos = "http://cran.us.r-project.org")
  library("tidystats")
}
if (!require("dplyr")) {
  install.packages("dplyr", repos = "http://cran.us.r-project.org")
  library("dplyr")
}
if (!require("vroom")) {
  install.packages("vroom", repos = "http://cran.us.r-project.org")
  library("vroom")
}
if (!require("rmdfiltr")) {
  install.packages("rmdfiltr", repos = "http://cran.us.r-project.org")
  library("rmdfiltr")
}
if (!require("tinytex")) {
  install.packages("tinytex", repos = "http://cran.us.r-project.org")
  library("tinytex")
}
if (!require("pwr")) {
  install.packages("pwr", repos = "http://cran.us.r-project.org")
  library("pwr")
}
if (!require("TOSTER")) {
  install.packages("TOSTER", repos = "http://cran.us.r-project.org")
  library("TOSTER")
}
if (!require("janitor")) {
  install.packages("janitor", repos = "http://cran.us.r-project.org")
  library("janitor")
}
if (!require("kableExtra")) {
  install.packages("kableExtra", repos = "http://cran.us.r-project.org")
  library("kableExtra")
}
if (!require("knitr")) {
  install.packages("knitr", repos = "http://cran.us.r-project.org")
  library("knitr")
}
if (!require("cowplot")) {
  install.packages("cowplot", repos = "http://cran.us.r-project.org")
  library("cowplot")
}
if (!require("labeling")) {
  install.packages("labeling", repos = "http://cran.us.r-project.org")
  library("labeling")
}
if (!require("corx")) {
  install.packages("corx", repos = "http://cran.us.r-project.org")
  library("corx")
}

checkpoint("2022-03-29") # Fill in the date of running this code. the checkpoint package thereby makes sure that every package required to run this script will be up to date.
```

```{r}

#References
r_refs("r-references.bib") 

# set seed
set.seed(20222303)
```

# Introduction

The integrity of a scientific field depends on the research practices that underlie how
knowledge is produced. In the past decade, practices that were previously common have
been scrutinized and called into question (Banks et al., 2016; Fiedler & Schwarz, 2016;
John et al., 2012). In Psychology, three practices especially seem to be stirring up the
debate surrounding the reliability of research findings, namely publication bias, selective
reporting, and lack of data sharing. As meta-scientists have started to examine the
consequences of these problematic practices, one concern that has been raised is the
impact that low replicability of research findings has on the public's trust in science (Anvari
& Lakens, 2018; Mede et al., 2021; Wingen et al., 2020). Pickett and Roche (2017) found
in a US sample that the general public judged a practice as selective reporting to be
morally unacceptable, and deserving of punishment. And yet, prevalence estimates of
publication bias, selective reporting, and lack of data sharing reveal these practices to be
common in psychological science. Given that a large part of scientific research is publicly
funded, the opinion of the public about these practices should be taken into account in
discussions about future implementations of tools to mitigate these problematic practices,
such as pre-registration, study registries, and data repositories. In the proposed study, we
assess public opinion about the moral acceptability of three research practices psychology
such that their views can be heard and considered in policy decisions.
In the current reform that the field of psychology is facing, the integrity of the scientific
literature plays a central role. Integrity refers to honesty, completeness, and soundness; in
science, this is reflected in honest, full reporting and verifiable steps in the process with
which research is conducted. Many practices and biases create a lack of completeness as
they prevent consumers of the work to see the whole picture (Vazire, 2017). For instance,
there are strong signs that publication bias is widely prevalent in the field of psychology
with over 90% of published results being in the ‘desirable’ direction (Fanelli, 2010; Scheel
et al., 2021). Researchers’ self-admission rates reveal that practices such as failing to
report all outcome measures, selectively leaving out parts of the data, or simply not
publishing when predictions are not confirmed, are common (Fiedler & Schwarz, 2016;
Fraser et al., 2018; John et al., 2012; Makel et al., 2021). Documented changes between
early reports of the research (e.g., a dissertation chapter) and the published version
uncover that selective reporting occurs in over half of all cases (Cairo et al., 2020; Franco
et al., 2014). The pervasiveness of these research practices stands in stark contrast with
principles formulated in codes of conduct, such as “Honesty in developing, undertaking,
reviewing, reporting and communicating research in a transparent, fair, full and unbiased
way” (ALLEA, 2017, p. 4).
Selectivity in reporting is likely driven by the current scientific incentive structure which
prioritizes novel findings that confirm predictions, while the importance of sharing null
results is undervalued. This leads to a strong motivation to spend one's time writing up
statistically significant results, irrespective of how important the research was judged to be
before looking at the outcomes (Greenwald, 1975). In addition, several studies have
shown that in psychology the majority of requested datasets cannot be obtained from the
original authors (Tedersoo et al., 2021; Vanpaemel et al., 2015). This is true even though
many journals in psychology require authors to share the data upon request, in line with 

A majority of participants in a survey by Picket and Roche (2017) judged selective
reporting to be morally unacceptable and believed that scientists who selectively report
should be fired and banned from receiving government funding. This suggests that public
opinion about these practices differs substantially from norms within the scientific
community, and raises the possibility that the scientific community would take problematic
research practices more seriously if public opinion was taken to heart.
However, researchers might be tempted to dismiss the opinion of the general public, as
they are relatively uninformed about the pressures that exist to publish positive results and
the effort involved in making data interpretable for other scientists. In our study, we
therefore examine moral acceptability judgments concerning selective reporting,
publication bias, and lack of data sharing, both before and after carefully informing
participants about possible underlying reasons for these behaviors, given the current
incentive system in psychological science. We also assess the trust participants have in
knowledge stemming from research within psychology both before and after this
explanation of the incentive system.
Furthermore, we ask participants to guess the prevalence of the three practices, and then
ask their opinion about meta-scientific estimates of the prevalence of these practices.
Together, the obtained public opinion about these research practices can inform future
policy decisions about ways to reduce this prevalence in the future, for example by
requiring preregistration of hypothesis tests to prevent selective reporting, registering
studies in a public study registry to prevent publication bias, and uploading data to a data
repository to prevent a lack of data sharing. In doing so, we aim to incorporate public
opinion in critical debates about research practices within psychology and create
awareness among scientists about how the general public perceives their practices.

In addition to the hypotheses below, the main aim of the proposed study is to collect
accurate descriptive statistics for the mean moral acceptability ratings from the general
public across four European countries concerning three research practices: selective
reporting, publication bias, and not sharing data. In addition, we ask participants whether
they believe tax money should be lowered if no responsibility is taken to resolve the
current practices by all stakeholders.
For selective reporting, publication bias, and not sharing data, we will assess the mean
moral acceptability, both before and after explaining the incentive structure in science that
leads to these practices. We will perform paired t-tests to examine whether participants
judge the three practices more acceptable after reading about the reward structure
compared to before. Based on previous work by Picket & Roche (2017), which shows that
the general public finds it morally unacceptable that researchers selectively report,
researchers might dismiss the opinion of the public because people are uninformed about
the incentive structures in science, and argue something like: "If the general public knew
how science operates, they would not find it as morally unacceptable that I do not share
data and selectively report or do not publish null results." We aim to directly test this
assumption. Our main interest is examining whether explaining the reward structure
makes people see this practice as more acceptable. We will perform three paired two-sided tests, as it might be that learning about the reward structure makes the public think it
is even less morally acceptable. We would like to make a claim that explanations of the
reward structure make any of the three practices (selective reporting, publication bias, and
not sharing data) more or less acceptable, so we correct our alpha level for 3 tests. We
expect that explanations of the incentive structure in science will somewhat increase moral
acceptability, but that participants on average continue to believe the three practices are
morally unacceptable (as indicated by averages lower than 50 on a 100 point scale after
learning about the incentive structure).
We aim to assess how learning about prevalence estimates based on meta-scientific
research impacts how much trust the general public has in knowledge generated by
psychological science by testing the difference in trust measured at the beginning and at
the end of our survey. We will test if their trust has been impacted by the information in the
survey by performing a paired t-test. We expect to observe a substantial drop in trust,
which provides an indication of a possible shift in opinion if current meta-scientific
prevalence estimates of these three scientific practices would become more widely known. 

In addition, for the three practices (selective reporting, publication bias, and not sharing data), we will
assess the participants’ guesses of the prevalence of each practice. We can compare this
guess of the public to current meta-scientific estimates of the prevalence of these practices
in psychology, to answer the question how closely aligned the prevalence estimates of the
general public are with meta-scientific estimates of the prevalence of these practices.
Based on our pilot data, we expect people to underestimate these practices (especially for
publication bias). After providing meta scientific estimates about the prevalence, we once
again ask participants how morally acceptable they believe this practice is, given the
prevalence. The response of people likely depends on how prevalent they judged the
practice to be, with the more the observed prevalence exceeds their own estimate, the
more immoral they believe the practice to be. Since we do not know how often people
think these practices happen (and thus, if they will believe they occur much less, or much
more, than their beliefs), these analyses are purely exploratory. 
Furthermore, the drop in trust before and after the prevalence estimates are communicated will depend
on the average trust in science. As we have collected data from 4 countries that differ in
trust in science, we will explore differences in the reduction of trust in knowledge
generated by psychological science across the four countries.
Finally, to map possible correlates of public opinion, we will explore individual differences that
may be associated with all the primary outcome measures (i.e., public opinion) in a
correlation table (e.g., correlations with familiarity with information from psychological
science, level of surprise at the current incentive structure in science, level of
understanding for a practice after reading the incentive structure explanation, agreement
with the statement that tax money should be lowered). We will also explore (with ANOVAs
and t-tests, without controlling error rates) differences on trust (both at the beginning and
end of survey), familiarity with psychological science, surprise at the current incentive
structure, and agreement with the statement that tax money should be lowered as a
function of country and education level.

```{r, data cleaning, include=FALSE, echo=TRUE, message=FALSE}
# Read in data. And remember to knit twice for cached objects
setwd("C:/Users/liamv/Desktop/School/Master Methodology & Statistics in Psychology/Blok 3/Internship")
file.name2 <- "Public Opinion Test File.csv"
data.raw2 <- read_csv(file.name2)

# Clean the data
data.clean2 <- data.raw2 %>%
  rename(
    start.date = StartDate,
    end.date = EndDate,
    status = Status,
    progress = Progress,
    duration = `Duration (in seconds)`,
    finished = Finished,
    recorded.date = RecordedDate,
    response.id = ResponseId,
    distribution.channel = DistributionChannel,
    user.language = UserLanguage,
    informed.consent = Q1.3,
    user.id = Q54,
    knowledge.trust.pre = Q3.1_1,
    psychology.familiar = Q3.2_1,
    psychology.familiar.choices = Q3.3,
    psychology.familiar.open = Q3.3_7_TEXT,
    scenario1.selective.reporting = Q4.3,
    scenario2.selective.reporting = Q4.5,
    acceptability.selective.reporting.pre = Q4.6_1,
    prevalence.estimate.selective.reporting = Q4.7_1,
    prevalence.acceptable.selective.reporting = Q4.9_1,
    scenario1.publication.bias = Q5.3,
    scenario2.publication.bias = Q5.5,
    acceptability.publication.bias.pre = Q5.6_1,
    prevalence.estimate.publication.bias = Q5.7_1,
    prevalence.acceptable.publication.bias = Q5.9_1,
    scenario.no.data = Q6.3,
    acceptability.no.data.pre = Q6.4_1,
    prevalence.estimate.no.data = Q6.5_1,
    prevalence.acceptable.no.data = Q6.7_1,
    click.first = `Q61_First Click`,
    click.last = `Q61_Last Click`,
    click.submit = `Q61_Page Submit`,
    click.count = `Q61_Click Count`,
    click.first2 = `Q7.2_First Click`,
    click.last2 = `Q7.2_Last Click`,
    click.submit2 = `Q7.2_Page Submit`,
    click.count2 = `Q7.2_Click Count`,
    surprised.incentive.struc = Q8.1_1,
    understanding.selective.reporting.post = Q9.2_1,
    acceptability.selective.reporting.post = Q9.3_1,
    understanding.publication.bias.post = Q10.2_1,
    acceptability.publication.bias.post = Q10.3_1,
    understanding.no.data.post = Q11.2_1,
    acceptability.no.data.post = Q11.3_1,
    knowledge.trust.post = Q12.3_1,
    cut.taxes.problems.not.solved = Q12.4_1,
    education.choice = Q12.5,
    education.open = Q12.5_5_TEXT,
    age = Q66,
    gender = Q67,
    comments.open = Q12.6,
    withdraw = Q64
  ) %>%
  # Remove first two rows that not contain any information
  slice(3:n()) %>%
  # Change type of variable (numeric)
  mutate_at(
    vars(
      progress,
      duration,
      knowledge.trust.pre,
      psychology.familiar,
      starts_with("responsibility"),
      starts_with("acceptability"),
      starts_with("prevalence"),
      starts_with("understanding"),
      surprised.incentive.struc,
      knowledge.trust.post,
      cut.taxes.problems.not.solved,
    ),
    as.numeric
  ) %>%
  # Change type of variable (factor)
  mutate_at(vars(age, education.choice, user.language, gender),as.factor) %>%
  # Change type of variable (datetime / dttm)
  mutate_at(vars(ends_with("date")), as.POSIXct) %>%
  # Change type of scenario variable (boolean)
  mutate_at(vars(starts_with("scenario")), list(~ . == 1)) %>%
  # Change type of permission variable (boolean)
  mutate_at(vars(informed.consent), list(~ . == 3)) %>%
  # Change type of finished variable (boolean)
  mutate_at(vars(finished), list(~ . == 1)) %>%
  # Change value names in 
  mutate(gender = recode(gender, 
                         '1' = "female", 
                         '2' = "male", 
                         '3' = "other", 
                         '4' = "not disclosed")) %>%
  mutate(age = recode(age, 
                      '1' = "<30", 
                      '2' = "30-39", 
                      '3' = "40-49", 
                      '4' = "50-59", 
                      '5' = ">60")) %>%
  mutate(education.choice = recode(education.choice, 
                                  '1' = "Low", 
                                  '2' = "Medium",
                                  '3' = "High",
                                  '4' = "Other")) %>%
  mutate(user.language = recode(user.language, 
                                  'NL' = "Dutch", 
                                  'ES-ES' = "Spanish",
                                  'IT' = "Italian",
                                  'PL' = "Polish")) %>%
  # Filter objects that (1) finished survey (2) gave permission (3) after the first of april (4) before fictional date.
  filter(finished & informed.consent & start.date >= "2022-03-31 00:00:00 CEST" & end.date <= "2022-12-31")

# Check the results of data cleaning:
lapply(data.clean2[sapply(data.clean2, is.factor)], levels)
lapply(data.clean2, is.numeric)
nrow(data.raw2) - nrow(data.clean2) #Should be 2, as first two rows in raw dataset contained no information.

# Filter the data according to exclusion criteria:

# Participants who are a researcher in the field of psychology are automatically excluded from the rest of the survey after selecting the option 5 in the familiar.choices question. 
data.filtered2 <- data.clean2 %>% 
  filter(!grepl('5', psychology.familiar.choices))

remove.psychology <- data.clean2 %>% 
  filter(grepl('5', psychology.familiar.choices)) %>%
  nrow()

N.after.excl.psy <- nrow(data.filtered2)

# Participants are also excluded when they indicate that they want to withdraw there answers after finishing the questionnaire.
data.filtered2 <- data.filtered2 %>% 	
  filter(!grepl(c("ritira|Ritira|retirar|Retirar|wycofaj|Wycofaj|intrekken|Intrekken"), withdraw))

remove.withdraw <- data.filtered2 %>% 	
  filter(grepl(c("ritira|Ritira|retirar|Retirar|wycofaj|Wycofaj|intrekken|Intrekken"), withdraw)) %>%
  nrow()

N.after.excl.withdraw <- nrow(data.filtered2)

# Create sample per country, for future analyses
data.filtered.NL <- subset(data.filtered2, user.language=='Dutch')
data.filtered.ES <- subset(data.filtered2, user.language=='Spanish')
data.filtered.PL <- subset(data.filtered2, user.language=='Polish')
data.filtered.IT <- subset(data.filtered2, user.language=='Italian')

```

```{r, missing data, include=FALSE, echo=TRUE, message=FALSE}

# Check for missing data: 5% missing data is calculated. Missing data can be deleted pairwise.
# Note: we do not count missing values in the open text variables, as answers here were not mandatory and could therefore significantly inflate the amount of missing data.
missing.data <- data.filtered2 %>%
  select(starts_with("acceptability"), 
         starts_with("knowledge"), 
         starts_with("prevalence"),
         starts_with("understanding"),
         cut.taxes.problems.not.solved) %>%
  is.na() %>%
  sum()

missing.data.percentage <- (missing.data / (nrow(missing.data) * ncol(missing.data))) * 100
print(missing.data.percentage) # percentage is < 5%, therefore margin of error for acceptability ratings is still sufficient at 2.50 (for 238 observations per country) and 1.25 (for 950 observations in total). Also the desired power is still sufficient.


```

```{r, sample size, include=FALSE, echo=TRUE, message=FALSE}

# Total sample size
N.before.excl.total <- data.clean2 %>% nrow()
N.after.excl.total <- data.filtered2 %>% nrow()

# Dutch sample size
N.before.excl.NL <- subset(data.clean2, user.language=='Dutch') %>% nrow()
N.after.excl.NL <- data.filtered.NL %>% nrow()

# Spanish sample size
N.before.excl.ES <- subset(data.clean2, user.language=='Spanish') %>% nrow()
N.after.excl.ES <- data.filtered.ES %>% nrow()

# Polish sample size
N.before.excl.PL <- subset(data.clean2, user.language=='Polish') %>% nrow()
N.after.excl.PL <- data.filtered.PL %>% nrow()

# Italian sample size
N.before.excl.IT <- subset(data.clean2, user.language=='Italian') %>% nrow()
N.after.excl.IT <- data.filtered.IT %>% nrow()

```

# Methods

## Participants

Our sample consisted of a total of `r N.before.excl.total` participants. In this sample, `r N.before.excl.NL` participants were Dutch, `r N.before.excl.ES` were Spanish, `r N.before.excl.PL` were Polish and `r N.before.excl.IT` were Italian. The data from the sample is quota-equivalent to the OCED statistics in the variables age, sex, and education level in two of the four European countries. `r remove.psychology` participants were excluded based on the criterium that they work as a researcher in the field of psychology. `r remove.withdraw` participants were excluded after indicating that they would like to withdraw from the study. Therefore, our final sample consisted of a total of `r after.excl.total` participants. Our sample sized was based on pre-planned accuracy estimates and our desired power. Based on our pilot data collected from Dutch participants through Prolific, and the largest
standard deviation observed for our dependent variables ($SD$ = 19.65 for the publication bias question), a sample of 250 per country will yield acceptability ratings with a 
margin of error of 2.43 on a 100 point scale, computed as $$1.96 * \frac{19.65}{\sqrt250}$$ with
an accuracy of 1.22 for $N$ = 1000 when averaging across countries computed as $$1.96 *
\frac{19.65}{\sqrt1000}$$. This means that in 95% of samples drawn using this procedure, the
population estimate will fall within a range of either 2.5 scale points higher or lower than
the true population estimate. We assume policy decisions would not meaningfully change
if the estimates we report are 2.5 points higher or lower on a 100 point scale (or 1.25 point
for the combined sample). Therefore, we believe the descriptives are accurate enough to
inform policy decisions. If we conservatively allow for 5% missing data, the margin of
errors are still sufficient at 2.50 (for 238 observations per country) and 1.25 (for 950
observations in total). 

```{r, power analysis, include=FALSE, echo=TRUE, warning=FALSE}

# Define our alpha level:
alpha.level <- 0.05 * 0.05

# Power analysis Two One-Sided T-tests
powerTOSTpaired.raw(alpha = (0.0025/3), N = 950, low_eqbound = -5,
                    high_eqbound = 5, sdif = 20.42)

# Power analysis T-test
power.analysis <- pwr.t.test(d = 5/20.42, n = 950, sig.level = 0.0025/3, type="paired",
alternative = "two.sided")

```

For our planned hypothesis tests H1 and H2 we have to specify a smallest effect size of
interest. How much more acceptable can the public judge selective reporting, publication
bias, and not sharing data, for researchers to argue that knowledge about the incentive
structure does *not* change moral acceptability judgments? And how much can trust
decrease after learning about prevalence estimates from meta-scientific work, without
considering this a meaningful change? We subjectively believe that a difference of 5 scale
points on a 100 point scale is a difference that is too small to matter in practice. Such a
change corresponds to a quarter of a standard deviation on the moral acceptability ratings
for publication bias, according to our pilot data. It seems difficult for researchers to argue
that the general public would not think selective reporting, publication bias, and not sharing
data are morally unacceptable if only they understood the incentive structures in science, if
the difference in their judgments is less than 5 scale points.
Given the effort required to collect data from a sample that is quota-equivalent to the
OCED statistics in the variables age, sex, and education level in two of the four European
countries, we do not predict it will be easy for other researchers to replicate our study. If
our findings are used to inform policy, we want to reduce the probability of erronous claims
beyond the default alpha level of 0.05. We therefore lower our alpha level to 0.0025, or
0.05 * 0.05, to yield the equivalent Type 1 error rate that would be achieved if two studies
(e.g., one original and one direct replication) with a default alpha of 0.05 had been
performed. As we will perform three paired t-tests (one for selective reporting, publication
bias, and not sharing data), we will effectively use an alpha level of 0.0025/3 for these
tests (but an alpha level of 0.0025 for H2). With a final sample size of 950 (allowing for 5%
missing values), we would have high (i.e., `r  sub("^0+", "",round(powerTOSTpaired.raw(alpha = (0.0025/3), N = 950, low_eqbound = -5, high_eqbound = 5, sdif = 20.42), 5))`) statistical power to conclude
the absence of a meaningful effect (set as a SESOI for a two-sided test of 5 on a 100 point
scale), given the largest observed standard deviation from our pilot data (for publication
bias, the standard deviation of the difference score was 20.42), assuming there was no
difference between pre- and post measures. For a sensitivity power analysis with 950 pairs of observations, we would also have high (i.e., `r sub("^0+", "",round(power.analysis$power, 5))`) power to detect a difference larger than 5 scale points (which
given the standard deviation of the difference score would equal a Cohen's $d_z$ = `r sub("^0+", "", round(power.analysis$d, 3))`).
After data collection, we applied our two exclusion criteria, namely that we would exclude researchers in the field of psychology, and participants who indicated that they wanted to withdraw from the study at the end of the survey.
Of the `r N.before.excl.total` participants, `r N.after.excl.total` ended up meeting these criteria. This sample consisted of `r N.after.excl.NL` dutch participants, of which `r round((sum(data.filtered.NL$gender == "female", na.rm = T) / N.after.excl.NL *100), digits = 2)`% were female, and `r round((sum(data.filtered.NL$gender == "male", na.rm = T) / N.after.excl.NL *100), digits = 2)`% were males. Among the `r N.after.excl.ES` Spanish participants, `r round((sum(data.filtered.ES$gender == "female", na.rm = T) / N.after.excl.ES *100), digits = 2)`% were female, and `r round((sum(data.filtered.ES$gender == "female", na.rm = T) / N.after.excl.ES *100), digits = 2)`% were males. The sample of Polish participants consisted of `r N.after.excl.PL` people, of which `r round((sum(data.filtered.PL$gender == "female", na.rm = T) / N.after.excl.PL *100), digits = 2)`% were female, and `r round((sum(data.filtered.PL$gender == "male", na.rm = T) / N.after.excl.PL *100), digits = 2)`% were males. Among the `r N.after.excl.IT` participants from Italy, `r round((sum(data.filtered.IT$gender == "female", na.rm = T) / N.after.excl.IT *100), digits = 2)`% were female, and `r round((sum(data.filtered.IT$gender == "male", na.rm = T) / N.after.excl.IT *100), digits = 2)`% were males. The distribution of age for all four countries are displayed in Figure \ref{fig:barplot_age}.

```{r, out.width = "100%", echo = F, fig.height = 3, fig.cap = "Barplot of the distribution of age per country.\\label{fig:barplot_age}"}
#Create barplot of age counts per country

ggplot(data=data.filtered2, aes(x=age, fill=user.language)) +
  geom_bar(colour = "black", position=position_dodge()) +
  ylim(0, NA) +
  xlab("Age") +
  ylab("Number of participants") +
  theme_cowplot() +
  scale_fill_discrete(name = "User language")

```

## Material

The study consisted of a survey with a single condition.The survey included,
among other measures, a moral acceptability rating that was assessed both
before and after information about the motivations of the different stakeholders (government,
journals, universities, and researchers) within the incentive structure
in science was given. A .qsf and corresponding Word file with the entire
survey as programmed in Qualtrics can be found on the osf project page
(<https://osf.io/7g96f/> under *study materials/Qualtrics*). The answers
were given on slider scales from 0 to 100 (numbers not visible to
participants) with labels at the extremes and in the middle, unless
otherwise specified below. Additionally, further information on the names and descriptions of the variables of this study can be found in a codebook on the osf project page. 

## Procedure

After giving informed consent and reading a short introduction to the
study, participants were asked how much they agree with the statement 'I
trust knowledge stemming from scientific research within Psychology'
(totally disagree - neither disagree nor agree - totally agree). The next survey item then
assessed participants' familiarity with knowledge stemming from Psychology:
'how familiar are you with knowledge stemming from Psychological
Science?' (not familiar at all - somewhat familiar - entirely familiar),
followed by a multiple choice question about how this familiarity came
about (read about it in the news / learned about it at school or university /
know someone who works in the field / learned about it at work / work as a researcher in psychology / other, please specify / not familiar). Participants then read an explanation of
one of the three practices, and answered two comprehension questions, on which they received direct feedback to increase comprehensibility. After that, they provided their opinion on '*how morally acceptable do you think it is
when scientists perform this practice?' (totally unacceptable - neutral -totally acceptable). They were then asked 'which percentage of scientists they think have performed this practice at least once in their career?' (0 - 100, each decimal indicated on the slider). After this estimation, meta scientific estimates of the prevalence of selective
reporting were provided, and participants were asked 'how morally acceptable do you think it is when ...% of scientists perform the practice at least once in their career?'. This sequence (explanation, comprehension
question(s), acceptability rating, prevalence estimate, and acceptability of meta scientific prevalence estimate) was then repeated for the remaining two practices.

What followed was a 400-word explanation of the motivations of the different stakeholders (government, journals,
universities and scientists) within the current scientific incentive
structure. For example, it was explained that in return for finance, the
government expects excellence and innovation, and that journals
strive for many subscribers and a large readership which can be
accomplished by publishing innovative and striking findings that confirm
the desired results. For universities, the text explained that reputation
is built by the success of their scientific staff in obtaining
competitive grants from the government or publishing papers. Lastly, for
scientists, it was explained that a successful career is best maintained
by doing innovative and striking research that confirms the desired
results, as this will lead to a higher chance of obtaining grants and/or
publishing papers; due to limited time and resources, this will leave
less time to document data for sharing or report studies that do not
show the desired results. Participants were not able to go to the next page for 100 seconds, to ensure that participants took the time to read and understand the explanation of the incentive structure withing science.

After reading this explanation text, participants were asked 'how surprised are you about
the current incentive structure within science, as explained above?'
(not at all surprised - neutral - entirely surprised). Next, participants were
asked 'given the knowledge you now have about the incentive structure
in science, how much understanding do you have for scientists who
perform (one of the three) practice?' (entirely no understanding - neutral - a lot of
understanding). Accompanying this question was a reminder on the definition of the practice at the top of the page. This question was added after piloting revealed that explaining motivations will likely have participants sympathize. This would then perhaps translate into their moral acceptability rating, even though one can have a lot of understanding for something and still think it is not morally acceptable. We therefore included this question about
understanding, which was then directly followed by 'Given the knowledge
you now have about the incentive structure in science, how morally
acceptable do you think it is when scientists perform this practice?'
(totally unacceptable - neutral - totally acceptable). This sequence (reminder, understanding rating and moral acceptability rating) was then repeated for the remaining two practices.

After these sequences, participants were asked five last questions. First, 
participants were again asked how much they agree with the statement 'I trust 
knowledge stemming from scientific research within Psychology' (totally disagree 
- neither disagree nor agree - totally agree). Second, participants were asked to 
indicate how much they agree with the statement *'If the different parties do not
take responsibility to resolve the three research practices (selective
reporting, publication bias, lack of data sharing), the tax money that
currently goes to scientific research within psychology should be
lowered'* (totally disagree - neither disagree nor agree - totally
agree). Third, participants were asked for their highest obtained
level of education (high school / post-secondary vocational education / 
higher professional education / university / other, namely: ...). Fourth, participants were asked about their age (younger than 30 years / between 30 and 39 / between 40 and 49 / between 50 and 59 / 60 years or older). Lastly, participants were asked about their gender (female / male / other / I would rather not say). At the end of the survey, there was space for participants to leave questions and/or comments. 
Participants were also informed that all stakeholders and other involved parties are constantly
reflecting on what can be done better in science, that this survey
itself was part of a project that received finance to make psychological
science more efficient and reliable, and that the participant's given
opinion would contribute to this end. We judged it was important to add this information, as to not leave participants with an inaccurately negative perception of the state of affairs in psychological science. Additionally, participants could leave their email addresses, to receive information about the results of the study. Finally, participants were given the opportunity to withdraw their answers from the study by typing "withdraw" in the box below.


# Results

Results of the planned analyses.

## Research Question 1

For "selective reporting", "publication bias", and "not sharing data",
we assessed the mean moral acceptability, both before and after
explaining the incentive structure in science that leads to these
practices. We performed three paired *t*-tests to determine whether
participants deemed the three practices more acceptable after reading
about the incentive structure. Based on previous work by [@pickett2018questionable] and a pilot study we conducted in 2020, which 
show that the general public finds it morally
unacceptable that researchers selectively report, researchers might
think: "If the general public knew how science operates and what incentive 
structures are in place, they would not
find it as morally unacceptable that I do not share data and selectively
report or do not publish null results." Therefore, one
of our main interests was knowing whether explaining the incentive structure
makes people see a practice as more acceptable. We tested a
two-sided test, as it might be that learning about the incentive structure
makes the public think it is even less morally acceptable. As we hoped to be able
to make a claim that explanations of the incentive structure makes a
practice more acceptable for any or all of the three practices, we
corrected our alpha level for 3 tests. We will effectively use an alpha level of 0.0025/3 for these
tests.

```{r, acceptability paired t-tests and SESOI tests, echo = TRUE, include = FALSE, warning = FALSE}
# Paired t-test for mean of moral acceptability pre and post survey, for the three practices. Alpha level correction of 0.0025/3. In addition, the equivalence test to examine if we can reject the smallest effect size of interest. 
# Note: the TOSTpaired.raw function as specified in the pre-registration is replaced by the tsum_TOST function, as this function has more utility.

## Tidy the data
acceptability.data2 <- data.filtered2 %>%
  # Select the right columns
  select(starts_with("acceptability"), user.id) %>%
  # Tidy the data
  pivot_longer(
    cols = starts_with("acceptability"),
    names_to = "name",
    values_to = "acceptability"
  ) %>%
  separate(
    col = "name",
    into = c("reaction", "topic1", "topic2", "time"),
    sep = "[.]"
  ) %>%
  select(-reaction) %>%
  unite(col = "topic", starts_with("topic"), sep = " ") %>%
  pivot_wider(names_from = time, values_from = acceptability) %>%
  mutate(difference = post - pre) %>%
  pivot_longer(
    cols = c("pre", "post"),
    names_to = "time",
    values_to = "acceptability"
  ) %>%
  mutate(time = factor(time, levels = c("pre", "post"))) %>%
  mutate(topic = str_replace(topic, "no data", "not sharing data")) %>%
  mutate(topic = factor(topic,
    levels = c(
      "selective reporting",
      "publication bias",
      "not sharing data"
    )
  )) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.

# OUT: data frame with: id, topic, difference pre/post, pre/post, accept score

# Find the group means
acceptability.data.means2 <- acceptability.data2 %>%
  group_by(topic, time) %>%
  summarise(grp_mean = mean(acceptability))

# And add them to the dataset
acceptability.data2 <- left_join(acceptability.data2, acceptability.data.means2)
rm(acceptability.data.means2)

# Compute statistics and add Tidystats of the t-tests
temp.accept.SR <- acceptability.data2 %>%
  select(user.id, topic, time, acceptability) %>%
  pivot_wider(names_from = time, values_from = acceptability) %>%
  filter(topic == "selective reporting")

paired.accept.SR <- t.test(temp.accept.SR$pre, temp.accept.SR$post, paired = T)

# Equivalence test to examine whether we can reject the smallest effect size of interest (SESOI).
equi.test.SR <- tsum_TOST(n1 = N.after.excl.total, n2 = N.after.excl.total, 
          m1 = mean(temp.accept.SR$pre), m2 = mean(temp.accept.SR$post), 
          sd1 = sd(temp.accept.SR$pre), sd2 = sd(temp.accept.SR$post),
          r12 = cor(temp.accept.SR$pre, temp.accept.SR$post),
          low_eqbound = -5, high_eqbound = 5)

# Compute statistics and add Tidystats of the t-tests
temp.accept.PB <- acceptability.data2 %>%
  select(user.id, topic, time, acceptability) %>%
  pivot_wider(names_from = time, values_from = acceptability) %>%
  filter(topic == "publication bias")

paired.accept.PB <- t.test(temp.accept.PB$pre, temp.accept.PB$post, paired = T)

# Equivalence test to examine whether we can reject the smallest effect size of interest (SESOI).
equi.test.PB <- tsum_TOST(n1 = N.after.excl.total, n2 = N.after.excl.total, 
          m1 = mean(temp.accept.PB$pre), m2 = mean(temp.accept.PB$post), 
          sd1 = sd(temp.accept.PB$pre), sd2 = sd(temp.accept.PB$post),
          r12 = cor(temp.accept.PB$pre, temp.accept.PB$post),
          low_eqbound = -5, high_eqbound = 5)

# Compute statistics and add Tidystats of the t-tests
temp.accept.NSD <- acceptability.data2 %>%
  select(user.id, topic, time, acceptability) %>%
  pivot_wider(names_from = time, values_from = acceptability) %>%
  filter(topic == "not sharing data")

paired.accept.NSD <- t.test(temp.accept.NSD$pre, temp.accept.NSD$post, paired = T)

# Equivalence test to examine whether we can reject the smallest effect size of interest (SESOI).
equi.test.NSD <- tsum_TOST(n1 = N.after.excl.total, n2 = N.after.excl.total, 
          m1 = mean(temp.accept.NSD$pre), m2 = mean(temp.accept.NSD$post), 
          sd1 = sd(temp.accept.NSD$pre), sd2 = sd(temp.accept.NSD$post),
          r12 = cor(temp.accept.NSD$pre, temp.accept.NSD$post),
          low_eqbound = -5, high_eqbound = 5, alpha = 0.0025)

# Add type/notes in add_stats
results <- list() %>%
  add_stats(paired.accept.SR) %>%
  add_stats(paired.accept.PB) %>%
  add_stats(paired.accept.NSD)


```

```{r, out.width = "100%", message = F, warning=F, fig.height = 3, fig.cap = "Moral acceptability before and after reading about incentive structure. \\label{fig:accept}", echo = F}
# Create the plot

acceptability.data2 %>%
  ggplot(aes(x = time, y = acceptability, fill = time)) +
  geom_violin(trim = FALSE) +
  facet_wrap(~topic) +
   stat_summary(
    fun.data = "mean_sdl", 
    fun.args = list(mult = 1), 
    geom = "pointrange", 
    colour = "black",
    shape = 21,
    fill = "white",
    show.legend = F) +
  theme_cowplot() +
  ylim(0, 105) +
  theme(legend.position = "none") +
  labs(
    x = "Before/after introducing information on incentive structure",
    y = "Moral acceptability (0-100)"
  )
```

```{r, table of descriptives, echo = TRUE, include = FALSE, warning = FALSE}
# Create table of descriptives of moral acceptability

# Select the right columns
accept.data <- data.filtered2 %>%
  select(starts_with("acceptability")) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.

# Create the rows         
accept.SR.pre <- c(mean(accept.data$acceptability.selective.reporting.pre), 
                   sd(accept.data$acceptability.selective.reporting.pre), 
                   range(accept.data$acceptability.selective.reporting.pre))

accept.SR.post <- c(mean(accept.data$acceptability.selective.reporting.post), 
                   sd(accept.data$acceptability.selective.reporting.post), 
                   range(accept.data$acceptability.selective.reporting.post))

accept.PB.pre <- c(mean(accept.data$acceptability.publication.bias.pre), 
                   sd(accept.data$acceptability.publication.bias.pre), 
                   range(accept.data$acceptability.publication.bias.pre))

accept.PB.post <- c(mean(accept.data$acceptability.publication.bias.post), 
                   sd(accept.data$acceptability.publication.bias.post), 
                   range(accept.data$acceptability.publication.bias.post))

accept.NSD.pre <- c(mean(accept.data$acceptability.no.data.pre), 
                   sd(accept.data$acceptability.no.data.pre), 
                   range(accept.data$acceptability.no.data.pre))

accept.NSD.post <- c(mean(accept.data$acceptability.no.data.post), 
                   sd(accept.data$acceptability.no.data.post), 
                   range(accept.data$acceptability.no.data.post))

# Creat data frame
accept.descriptives <- as.data.frame(rbind(accept.SR.pre, accept.SR.post, 
                                           accept.PB.pre, accept.PB.post, 
                                           accept.NSD.pre, accept.NSD.post)) %>%
  rename(
    Mean = V1, 
    SD = V2,
    Lower = V3,
    Upper = V4,
   ) 

rownames(accept.descriptives) <- c("Acceptability SR (pre)",
              "Acceptability SR (post)",
              "Acceptability PB (pre)",
              "Acceptability PB (post)",
              "Acceptability NSD (pre)",
              "Acceptability NSD (post)")

descriptives.APA <- papaja::apa_table(accept.descriptives,
                   caption = "Descriptives of moral acceptability for the three practices",
                  note = "SR = selective reporting, PB = publication bias and NSD = not sharing data. Descriptives are                           assessed both before (pre) and after (post) the survey.",
                  placement = "h",
                  escape = T)
```

Figure \ref{fig:accept} shows the acceptability ratings for all three practices. Acceptability of selective reporting was not rated higher after reading about the incentive structure in science compared to before, $M_{pre}$ = `r mean(temp.accept.SR$pre)`, $M_{post}$ = `r mean(temp.accept.SR$post)`,
`r papaja::apa_print(paired.accept.SR)$full_result`. Based on an equivalence test, we did not reject the presence of effects more extreme than -0.5 to 0.5, so we can act (with an error rate of .0025) as if the effect, if any, is less extreme than our equivalence range. Furthermore, acceptability of publication bias was also not rated higher after reading about the incentive structure in science compared to before, $M_{pre}$ = `r mean(temp.accept.PB$pre)`, $M_{post}$ = `r mean(temp.accept.PB$post)`, `r papaja::apa_print(paired.accept.PB)$full_result`. Again, we did not reject the presence of effects more extreme than -0.5 to 0.5. Lastly, acceptability of scientists not sharing their data was also not rated higher after reading about the incentive structure in science compared to before, $M_{pre}$ = `r mean(temp.accept.NSD$pre)`, $M_{post}$ = `r mean(temp.accept.NSD$post)`, `r papaja::apa_print(paired.accept.NSD)$full_result`. Once more, based on an equivalence test, we did not reject the presence of effects more extreme than -0.5 to 0.5. \textcolor{red}{Alter the results on the basis of the new data}

Table \ref{tab:table of descriptives} shows the mean, standard deviation and range of the acceptability ratings for all three practices. These descriptives can be used to inform policy makers about the public opinion on, and the severity of, the three practices and could therefore be relevant for policy decisions about whether action should be taken to prevent the occurrence of these issues. 

`r descriptives.APA`

## Research Question 2

```{r, include = F}
# Reshape the data
trust.data <- data.filtered2 %>%
  # Select correct variables
  select(response.id, knowledge.trust.pre, knowledge.trust.post) %>%
  # Compute differences pre/post
  mutate(diff = knowledge.trust.post - knowledge.trust.pre) %>%
  # Tidy the data
  pivot_longer(-c(response.id, diff), names_to = "time", values_to = "trust") %>%
  mutate(
    time = str_remove(time, "knowledge.trust."),
    time = factor(time, levels = c("pre", "post"))
  ) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
# Add group means
trust.data.means <- trust.data %>%
  group_by(time) %>%
  summarise(grp_mean = mean(trust))
# Join the data
trust.data <- left_join(trust.data, trust.data.means)
rm(trust.data.means)

# Add tidystats t-test
temp <- trust.data %>%
  select(response.id, time, trust) %>%
  pivot_wider(names_from = time, values_from = trust)

t1.trust <- t.test(temp$pre, temp$post, paired = T)
rm(temp)

results <- results %>%
  add_stats(t1.trust)

# Calculation of Cohen's d
before <- trust.data$trust[trust.data$time == "pre"]
after <- trust.data$trust[trust.data$time == "post"]
diff <- after - before
(mean(before) - mean(after)) / sd(diff)
```

This research question focused on several general (i.e., not specific to the three
separate research reporting practices) measures to assess the public opinion of psychological
science.
We assessed how much trust the public has in knowledge forwarded by
psychological science and how much this trust has been impacted by the information in the
survey by comparing the trust measured at the beginning and the end of the survey.
Figure \ref{fig:trust} shows that participants' trust in knowledge
coming from psychological science had declined at the end of the survey
(`r papaja::apa_print(t1.trust)$full_result`, *d* = `r (mean(before) - mean(after)) / sd(diff)`). \textcolor{red}{Alter the results on the basis of the new data}


```{r, out.width="100%", message = F, fig.height = 3, echo = F, fig.cap = "Trust in knowledge coming from psychological science at the beginning and at the end of the survey.\\label{fig:trust}"}
# Create plot
trust.data %>%
  ggplot(aes(x = time, y = trust, fill = time)) +
  geom_violin(show.legend = F) +
  stat_summary(
    fun.data = "mean_sdl", 
    fun.args = list(mult = 1), 
    geom = "pointrange", 
    color = "black",
    fill = "white",
    shape = 21,
    show.legend = F) +
  theme_cowplot() +
  ylim(c(0, 105))+
  labs(
    x = "",
    y = "Trust in Psychology"
  )
```

## Research Question 3

For the three practices (selective reporting, publication bias, and
not sharing data), we assessed the participants' guess of
prevalence of the practice. We compared this guess of the public across the four countries
to current meta scientific estimates of the prevalence of these
practices in psychology, to answer the question: How closely aligned are
the prevalence estimates of the general public with meta scientific
estimates of the prevalence of these practices? After providing meta
scientific estimates about the prevalence, we asked participants'
moral acceptability rating of this given prevalence. How morally
acceptable do participants think a practice is after they hear how often
it occurs? It makes sense to assume that they would respond differently
depending on what they believe is the frequency of each practice. Since
we had no previous knowledge on how often people think these practices
happen (and thus, if they will believe they occur much less, or much
more, than their beliefs), these analyses were purely exploratory. We
computed a difference score between the meta scientific prevalence and
participants' guess of prevalence to assess whether participants over-
or underestimated the prevalence. We used this difference score to
predict moral acceptability after post survey.

Figure \ref{fig:prevalence} depicts participants' acceptability ratings
as a function of their prevalence guess of the three practices. The
pattern seems to be that when participants underestimate the prevalence
of a given practice, their moral acceptability of that practice is lower
compared to when they overestimate the prevalence. \textcolor{red}{Alter the results on the basis of the new data}

```{r, out.width = "100%", fig.height = 3, fig.cap = "Acceptability rating by prevalence guess.\\label{fig:prevalence}", echo = F, warning = FALSE}
# Reshape the data
data.filtered2 %>%
  # Select correct variables
  select(starts_with("prevalence"), response.id) %>%
  # Put the three "topics" in the same column and improve the names
  pivot_longer(col = -response.id,names_to = "name", values_to = "value") %>%
  separate(
    col = "name",
    into = c("prevalence", "metric", "topic"),
    sep = "[.]",
    extra = "merge"
  ) %>%
  select(-prevalence) %>%
  mutate(topic = str_remove(topic, "scientists.")
  %>% str_replace("[.]", " ")
    %>% str_replace("no data", "not sharing data")) %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  # Add the true estimates (given in survey)
  mutate(
    estimate.true = rep(c(45, 93, 62), n() / 3),
    estimate.diff = estimate - estimate.true
  ) %>%
  mutate(diff.bool = ifelse(estimate.diff > 0,
    "overestimate",
    "underestimate"
  )) %>%
  mutate("topic" = factor(topic,
    levels = c(
      "selective reporting",
      "publication bias",
      "not sharing data"
    )
  )) %>%
  filter(complete.cases(.)) %>% #Pairwise deletion of rows with missing data. 
  # Create plot
  ggplot(aes(y = acceptable, x = estimate, color = diff.bool)) +
  geom_point() +
  facet_wrap(~topic) +
  theme_cowplot() +
  geom_smooth(method = "lm", formula = "y~x") +
  labs(color = "")+
  labs(
    x = "Prevalence guess",
    y = "Moral acceptability (0-100)"
  )

```


## Research Question 4

```{r, eval = FALSE, echo = TRUE, include = FALSE, warning = FALSE}
#Paired t-test for trust in knowledge stemming from psychological science pre and post survey per country. Alpha level correction of 0.0025/3

###### Netherlands ######

# Reshape the data
trust.data.NL <- data.filtered.NL %>%
  # Select correct variables
  select(response.id, knowledge.trust.pre, knowledge.trust.post) %>%
  # Compute differences pre/post
  mutate(diff = knowledge.trust.post - knowledge.trust.pre) %>%
  # Tidy the data
  pivot_longer(-c(response.id, diff), names_to = "time", values_to = "trust") %>%
  mutate(
    time = str_remove(time, "knowledge.trust."),
    time = factor(time, levels = c("pre", "post"))
  ) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.

# Add group means
trust.data.means.NL <- trust.data.NL %>%
  group_by(time) %>%
  summarise(grp_mean = mean(trust))
# Join the data
trust.data.NL <- left_join(trust.data.NL, trust.data.means.NL)
rm(trust.data.means.NL)

# Add tidystats t-test
temp.trust.NL <- trust.data.NL %>%
  select(response.id, time, trust) %>%
  pivot_wider(names_from = time, values_from = trust)

t1.trust.NL <- t.test(temp.trust.NL$pre, temp.trust.NL$post, paired = T)


results <- results %>%
  add_stats(t1.trust.NL)

# Calculation of Cohen's d
before.NL <- trust.data.NL$trust[trust.data.NL$time == "pre"]
after.NL <- trust.data.NL$trust[trust.data.NL$time == "post"]
diff.NL <- after.NL - before.NL
Cohen.NL <- (mean(before.NL) - mean(after.NL)) / sd(diff.NL)



###### Italy ######

# Reshape the data
trust.data.IT <- data.filtered.IT %>%
  # Select correct variables
  select(response.id, knowledge.trust.pre, knowledge.trust.post) %>%
  # Compute differences pre/post
  mutate(diff = knowledge.trust.post - knowledge.trust.pre) %>%
  # Tidy the data
  pivot_longer(-c(response.id, diff), names_to = "time", values_to = "trust") %>%
  mutate(
    time = str_remove(time, "knowledge.trust."),
    time = factor(time, levels = c("pre", "post"))
  ) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
# Add group means
trust.data.means.IT <- trust.data.IT %>%
  group_by(time) %>%
  summarise(grp_mean = mean(trust))
# Join the data
trust.data.IT <- left_join(trust.data.IT, trust.data.means.IT)
rm(trust.data.means.IT)

# Add tidystats t-test
temp.trust.IT <- trust.data.IT %>%
  select(response.id, time, trust) %>%
  pivot_wider(names_from = time, values_from = trust)

t1.trust.IT <- t.test(temp.trust.IT$pre, temp.trust.IT$post, paired = T)

results <- results %>%
  add_stats(t1.trust.IT)

# Calculation of Cohen's d
before.IT <- trust.data.IT$trust[trust.data.IT$time == "pre"]
after.IT <- trust.data.IT$trust[trust.data.IT$time == "post"]
diff.IT <- after.IT - before.IT
Cohen.IT <- (mean(before.IT) - mean(after.IT)) / sd(diff.IT)



###### Spain ######

# Reshape the data
trust.data.ES <- data.filtered.ES %>%
  # Select correct variables
  select(response.id, knowledge.trust.pre, knowledge.trust.post) %>%
  # Compute differences pre/post
  mutate(diff = knowledge.trust.post - knowledge.trust.pre) %>%
  # Tidy the data
  pivot_longer(-c(response.id, diff), names_to = "time", values_to = "trust") %>%
  mutate(
    time = str_remove(time, "knowledge.trust."),
    time = factor(time, levels = c("pre", "post"))
  ) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
# Add group means
trust.data.means.ES <- trust.data.ES %>%
  group_by(time) %>%
  summarise(grp_mean = mean(trust))
# Join the data
trust.data.ES <- left_join(trust.data.ES, trust.data.means.ES)
rm(trust.data.means.ES)

# Add tidystats t-test
temp.trust.ES <- trust.data.ES %>%
  select(response.id, time, trust) %>%
  pivot_wider(names_from = time, values_from = trust)

t1.trust.ES <- t.test(temp.trust.ES$pre, temp.trust.ES$post, paired = T)

results <- results %>%
  add_stats(t1.trust.ES)

# Calculation of Cohen's d
before.ES <- trust.data.ES$trust[trust.data.ES$time == "pre"]
after.ES <- trust.data.ES$trust[trust.data.ES$time == "post"]
diff.ES <- after.ES - before.ES
Cohen.ES <- (mean(before.ES) - mean(after.ES)) / sd(diff.ES)



###### Poland ######

# Reshape the data
trust.data.PL <- data.filtered.PL %>%
  # Select correct variables
  select(response.id, knowledge.trust.pre, knowledge.trust.post) %>%
  # Compute differences pre/post
  mutate(diff = knowledge.trust.post - knowledge.trust.pre) %>%
  # Tidy the data
  pivot_longer(-c(response.id, diff), names_to = "time", values_to = "trust") %>%
  mutate(
    time = str_remove(time, "knowledge.trust."),
    time = factor(time, levels = c("pre", "post"))
  ) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
# Add group means
trust.data.means.PL <- trust.data.PL %>%
  group_by(time) %>%
  summarise(grp_mean = mean(trust))
# Join the data
trust.data.PL <- left_join(trust.data.PL, trust.data.means.PL)
rm(trust.data.means.PL)

# Add tidystats t-test
temp.trust.PL <- trust.data.PL %>%
  select(response.id, time, trust) %>%
  pivot_wider(names_from = time, values_from = trust)

t1.trust.PL <- t.test(temp.trust.PL$pre, temp.trust.PL$post, paired = T)

results <- results %>%
  add_stats(t1.trust.PL)

# Calculation of Cohen's d
before.PL <- trust.data.PL$trust[trust.data.PL$time == "pre"]
after.PL <- trust.data.PL$trust[trust.data.PL$time == "post"]
diff.PL <- after.PL - before.PL
Cohen.PL <- (mean(before.PL) - mean(after.PL)) / sd(diff.PL)
```

```{r, out.width = "100%", eval = FALSE, message = F, warning=F, fig.height = 3, fig.cap = "Trust in knowledge stemming from psychological science before and after reading about incentive structure for all countries. \\label{fig:trust_country}", echo = F}

###### Netherlands ######

# Create plot
trust.plot.NL <- ggplot(data = trust.data.NL, aes(x = time, y = trust, fill = time)) +
  geom_violin(show.legend = F) +
  stat_summary(
    fun.data = "mean_sdl", 
    fun.args = list(mult = 1), 
    geom = "pointrange", 
    color = "black",
    fill = "white",
    shape = 21,
    show.legend = F) +
  theme_cowplot() +
  ylim(c(0, 105))+
  labs(
    x = "",
    y = "Trust"
  )

###### Italy ######

# Create plot
trust.plot.IT <- ggplot(data = trust.data.IT, aes(x = time, y = trust, fill = time)) +
  geom_violin(show.legend = F) +
  stat_summary(
    fun.data = "mean_sdl", 
    fun.args = list(mult = 1), 
    geom = "pointrange", 
    color = "black",
    fill = "white",
    shape = 21,
    show.legend = F) +
  theme_cowplot() +
  ylim(c(0, 105))+
  labs(
    x = "",
    y = "Trust"
  )

###### Spain ######

# Create plot
trust.plot.ES <- ggplot(data = trust.data.ES, aes(x = time, y = trust, fill = time)) +
  geom_violin(show.legend = F) +
  stat_summary(
    fun.data = "mean_sdl", 
    fun.args = list(mult = 1), 
    geom = "pointrange", 
    color = "black",
    fill = "white",
    shape = 21,
    show.legend = F) +
  theme_cowplot() +
  ylim(c(0, 105))+
  labs(
    x = "",
    y = "Trust"
  )

###### Poland ######

# Create plot
trust.plot.PL <- ggplot(data = trust.data.PL, aes(x = time, y = trust, fill = time)) +
  geom_violin(show.legend = F) +
  stat_summary(
    fun.data = "mean_sdl", 
    fun.args = list(mult = 1), 
    geom = "pointrange", 
    color = "black",
    fill = "white",
    shape = 21,
    show.legend = F) +
  theme_cowplot() +
  ylim(c(0, 105))+
  labs(
    x = "",
    y = "Trust"
  )

ggarrange(trust.plot.NL, trust.plot.IT, trust.plot.ES, trust.plot.PL, 
          labels = c("Netherlands", "Italy", "Spain", "Poland"),
          ncol = 2, nrow = 2)


```

## Research Question 5

```{r, correlation table individual differences, echo = F, warning = F, results = "asis"}
# Prepare data
individual.diff.data <- data.filtered2 %>%
  select(
    knowledge.trust.pre, knowledge.trust.post, psychology.familiar, 
    surprised.incentive.struc, cut.taxes.problems.not.solved,
    acceptability.selective.reporting.post, acceptability.publication.bias.post, 
    acceptability.no.data.post, acceptability.selective.reporting.pre, acceptability.publication.bias.pre, 
    acceptability.no.data.pre, understanding.selective.reporting.post, understanding.publication.bias.post, 
    understanding.no.data.post
    ) %>%
  rename(
    'Trust in psychology (pre)' = knowledge.trust.pre,
    'Trust in psychology (post)' = knowledge.trust.post,
    'Familiarity with psychology' = psychology.familiar,
    'Surprise incentive structure' = surprised.incentive.struc,
    'Agreement tax cuts' = cut.taxes.problems.not.solved,
    'Acceptability pre (selective reporting)' = acceptability.selective.reporting.pre,
    'Acceptability pre (publication bias)' = acceptability.publication.bias.pre,
    'Acceptability pre (not sharing data)' = acceptability.no.data.pre,
    'Acceptability post (selective reporting)' = acceptability.selective.reporting.post,
    'Acceptability post (publication bias)' = acceptability.publication.bias.post,
    'Acceptability post (not sharing data)' = acceptability.no.data.post,
    'Understanding (selective reporting)' = understanding.selective.reporting.post,
    'Understanding (publication bias)' = understanding.publication.bias.post,
    'Understanding (not sharing data)' = understanding.no.data.post,
    'Age' = age
  ) 

# Create correlation table  
corr.individual.diff.mat <- 
  corx(individual.diff.data, 
       triangle = "lower", round = 2,
       remove_lead = T)

# No stars, as these results are purely exploratory.

# Make table APA
corr.individual.diff.APA <- papaja::apa_table(corr.individual.diff.mat$apa,
                                  caption = "Correlations between 
                                  possible correlates of public 
                                  opinion.",
                                  escape = T, landscape = T)


```

To map possible correlates of public opinion, we explored individual differences that may be associated with all the primary outcome measures (i.e., public opinion) in a correlation table. Table \ref{tab:correlation table individual differences} shows these correlations. A moderate correlation was found between trust in psychology at the beginning and end of the study. In addition, a moderate correlation was found between the acceptability ratings of selective reporting and acceptability ratings of publication bias.. The acceptability ratings of publication bias and the acceptability ratings of not sharing data were also moderately correlated. \textcolor{red}{Alter the results on the basis of the new data}

`r corr.individual.diff.APA`

```{r, ANOVAs individual differences, warning = F, message = F, include = F}

# Two-way Anova 1: differences on trust as a function of both user language and educational level.

trust.data.aov <- data.filtered2 %>%
  select(response.id, user.language, education.choice, knowledge.trust.pre, knowledge.trust.post) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
  
aov.trust.pre <- aov(knowledge.trust.pre ~  user.language + education.choice, data = trust.data.aov)

aov.trust.post <- aov(knowledge.trust.post ~  user.language + education.choice, data = trust.data.aov)

# Two-way Anova 2: differences on familiarity with psychological science as a function of both user language and educational level.

familiar.data.aov <- data.filtered2 %>%
  select(response.id, user.language, education.choice, psychology.familiar) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
  
aov.familiar <- aov(psychology.familiar ~  user.language + education.choice, data = familiar.data.aov)

# Two-way Anova 3: differences on surprise at incentive structure as a function of both user language and educational level.

surprise.data.aov <- data.filtered2 %>%
  select(response.id, user.language, education.choice, surprised.incentive.struc) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
  
aov.surprise <- aov(surprised.incentive.struc ~ user.language + education.choice, data = surprise.data.aov)

# Two-way Anova 4: differences on agreement with the statement that tax money should be lowered as a function of both user language and educational level.

agreement.data.aov <- data.filtered2 %>%
  select(response.id, user.language, education.choice, cut.taxes.problems.not.solved) %>%
  filter(complete.cases(.)) #Pairwise deletion of rows with missing data.
  
aov.agreement <- aov(cut.taxes.problems.not.solved ~  user.language + education.choice, data = agreement.data.aov)

summary(aov.agreement)

# Add stats
results <- results %>%
  add_stats(aov.trust.pre) %>%
  add_stats(aov.trust.post) %>%
  add_stats(aov.familiar) %>%
  add_stats(aov.surprise) %>%
  add_stats(aov.agreement)

```

```{r, eval = F}
We also explored differences on trust (both at the beginning and end of
survey), familiarity with psychological science, surprise at the current
incentive structure, and agreement with the statement that tax money
should be lowered as a function of education level and user language. The first ANOVA revealed that there was no statistically significant difference on trust in psychological science before the survey on both  (`r papaja::apa_print(aov.trust.pre)$full_result$user_language`) and educational level (`r papaja::apa_print(aov.trust.pre)$full_result$education_choice`). No significant differences were found for trust in psychological science after the survey (`r papaja::apa_print(aov.trust.post)$full_result$user_language`) & (`r papaja::apa_print(aov.trust.post)$full_result$education_choice`)  Another ANOVA revealed that there was no statistically significant difference on familiarity with knowledge stemming from psychological science on user language (`r papaja::apa_print(aov.familiar)$full_result$user_language`), however differences were found on educational level (`r papaja::apa_print(aov.familiar)$full_result$education_choice`). The third ANOVA revealed that no statistically significant difference in surprise about the incentive structure within science was found on user language (`r papaja::apa_print(aov.surprise)$full_result$user_language`), however differences were found on educational level once more (`r papaja::apa_print(aov.surprise)$full_result$education_choice`). A final ANOVA revealed that no statistically significant differences were found on agreeance with the statement that tax money should be lowered if no responsibility is taken to resolve the issues mentioned before on user language (`r papaja::apa_print(aov.agreement)$full_result$user_language`), however significant differences were found on educational level (`r papaja::apa_print(aov.agreement)$full_result$education_choice`). 

 \textcolor{red}{Alter the results on the basis of the new data}
```

# Discussion

In the scientific field, and specifically the scientific field of psychology, concerns have been raised that several research practices undermine the fields' reliability and integrity. In particular, selective reporting, publication bias and lack of data sharing have been stirring up the debate surrounding the research findings. Although the opinions of scientists regarding these practices have been frequently evaluated, the opinion of the public has oftentimes been neglected, even though they are the ones that arguably have the most right to the knowledge shares by scientists. In the present study, we assessed the opinion of `r N.after.excl.total` participants from four different European countries (i.e., The Netherlands, Poland, Italy and Spain) regarding selectively reporting, publication bias and lack of data sharing with a survey. First, with regard to the question whether the public found the three practices more morally acceptable after explaining the incentive structure in science, we found that the public found the three practices more morally acceptable after the incentive structure was explained. However, the public still thought that the practices were not morally acceptable, as the mean moral acceptability post survey was still below the cut-off point of 50. These results are in line with our hypothesis. 
Furthermore, in regard to the question whether the trust of the public in knowledge stemming from psychological science, we found that the the average trust of the public decreased as a result of the provided meta scientific prevalence estimates of the three practices.
We also explored how closely aligned the guesses of the public's prevalence estimates are with the meta scientific estimates of these practices and how morally acceptable the public found the three practices after learning about the meta scientific prevalence estimates. We found that the moral acceptability decreased after meta scientific estimates were provided. Furthermore, participants generally overestimated the prevalence of selective reporting and underestimated the prevalence of publication bias and lack of data sharing. In addition, the public thought that the three research practices were morally more acceptable when they overestimated the meta scientific prevalence of these practices compared to when they underestimated the meta scientific prevalence.
In addition, we explored differences in the drop of trust after reading about the incentive structure for the four participating countries, as these countries differ in their trust in science. We found that the Polish public had the lowest trust in knowledge stemming from psychological science before and after the survey. However, the average trust decreased the most after reading about the incentive structure for the Italian public opinion.
Lastly, to map possible correlates of public opinion, we explored individual differences that
may be associated with all the primary outcome measures (i.e., public opinion) in a correlation table. We found significant correlation between trust in knowledge stemming from psychological science after the survey and agreeance with the statement that tax money should be lowered if no responsibility is taken to resolve these practices. 
This study makes several noteworthy contributions to the literature of the integrity of the scientific research field. First, we found that the general public judged the practice of selective reporting morally unacceptable, which is in accordance to the findings from the study by [@pickett2018questionable]. In addition, we found that the general public also judged publication bias and lack of data sharing morally unacceptable.
Potential limitations of this study should be considered. First, this study examined the public opinion of four different countries, who are all exposed to a different educational system. We experienced some difficulty matching the educational levels of the participants from different countries. Also, our study relies on participants recruited from populations that fit the White, Educated, Industrialized, Rich, and Democratic (WEIRD) criteria. This is therefore a threat to the external validity of the study.
Strengths of the current study include (...) 

\textcolor{red}{an outline of the discussion with made-up results}

\newpage

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```

## References

::: {#refs custom-style="Bibliography"}
:::

\endgroup

## Data analysis

We used `r cite_r("r-references.bib")` for all our analyses.
